{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Moved to 256x256 with the model architecture, kept 200 as latent_dim since it gives better results, batch_size is 32 - max for this resolution, and point of comparison to smaller resolutions. By judging the pictures from smaller resolutions created with the same batch size I think that being able to have a bigger one here would give better pictures.\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchviz import make_dot\n",
    "from torchinfo import summary\n",
    "# Define constants\n",
    "IMG_SIZE = 256  # Set to 256x256 for the new output size\n",
    "LATENT_DIM = 200  # Updated latent dimension\n",
    "BATCH_SIZE = 32  # Updated batch size\n",
    "EPOCHS = 100\n",
    "\n",
    "epoch_times = []  # List to store time taken for each epoch\n",
    "d_loss_values = []\n",
    "g_loss_values = []\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.init_size = IMG_SIZE // 16  # Adjusted for 256x256 images\n",
    "        self.l1 = nn.Sequential(nn.Linear(LATENT_DIM, 256 * self.init_size ** 2))\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(256, 256, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256, 0.8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(256, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(64, 32, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32, 0.8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 3, 3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.l1(z)\n",
    "        out = out.view(out.shape[0], 256, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, 1024, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024 * 8 * 8, 1), \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        validity = self.model(img)\n",
    "        return validity\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    ConcatDataset([datasets.Flowers102(root='../../data/flowers', split='train', download=True, transform=transform),\n",
    "                   datasets.Flowers102(root='../../data/flowers', split='val', download=True, transform=transform),\n",
    "                   datasets.Flowers102(root='../../data/flowers', split='test', download=True, transform=transform)]),\n",
    "    batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "\n",
    "# Initialize models\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "# Generate and save summary for Generator model\n",
    "generator_summary = summary(generator, input_size=(1, LATENT_DIM), verbose=2, col_names=[\"input_size\", \"output_size\", \"num_params\", \"kernel_size\", \"mult_adds\"])\n",
    "with open(\"generator_summary_256.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(str(generator_summary))\n",
    "\n",
    "# Generate and save summary for Discriminator model\n",
    "discriminator_summary = summary(discriminator, input_size=(1, 3, IMG_SIZE, IMG_SIZE), verbose=2, col_names=[\"input_size\", \"output_size\", \"num_params\", \"kernel_size\", \"mult_adds\"])\n",
    "with open(\"discriminator_summary_256.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(str(discriminator_summary))\n",
    "\n",
    "# Visualize the Generator Model\n",
    "z_sample = torch.randn(1, LATENT_DIM).to(device)  # A single sample input for the generator\n",
    "gen_sample_output = generator(z_sample)\n",
    "gen_dot = make_dot(gen_sample_output, params=dict(generator.named_parameters()))\n",
    "gen_dot.render(\"generator_architecture_256\", format=\"png\")  # Save the visualization as a PNG file\n",
    "\n",
    "# Visualize the Discriminator Model\n",
    "real_img_sample = torch.randn(1, 3, IMG_SIZE, IMG_SIZE).to(device)  # A single sample input for the discriminator\n",
    "disc_sample_output = discriminator(real_img_sample)\n",
    "disc_dot = make_dot(disc_sample_output, params=dict(discriminator.named_parameters()))\n",
    "disc_dot.render(\"discriminator_architecture_256\", format=\"png\")  # Save the visualization as a PNG file\n",
    "# Loss function\n",
    "adversarial_loss = nn.BCELoss()\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0005, betas=(0.5, 0.999))\n",
    "\n",
    "# Training\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_start_time = time.time()  # Start time for the epoch\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "        \n",
    "                # Calculate and print GPU memory usage\n",
    "        allocated_memory = torch.cuda.memory_allocated() / (1024 ** 3)  # Convert to GB\n",
    "        reserved_memory = torch.cuda.memory_reserved() / (1024 ** 3)    # Convert to GB\n",
    "        # Configure input\n",
    "        real_imgs = imgs.to(device)\n",
    "        batch_size = real_imgs.size(0)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = torch.ones(batch_size, 1, requires_grad=False).to(device)\n",
    "        fake = torch.zeros(batch_size, 1, requires_grad=False).to(device)\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        z = torch.randn(batch_size, LATENT_DIM).to(device)\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z)\n",
    "\n",
    "        # Loss for real images\n",
    "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
    "        # Loss for fake images\n",
    "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
    "        # Total discriminator loss\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Loss for fake images with flipped labels\n",
    "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch}/{EPOCHS}] [Batch {i}/{len(dataloader)}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}]\")\n",
    "        print(f\"[GPU Memory Allocated: {allocated_memory:.2f} GB] [GPU Memory Reserved: {reserved_memory:.2f} GB]\")\n",
    "\n",
    "    # Save sample images\n",
    "    if epoch % 2 == 0:\n",
    "        save_image(gen_imgs.data[:25], f\"images/{epoch}_DCGAN_flowers2_256.png\", nrow=5, normalize=True)\n",
    "        # Save the model\n",
    "        torch.save(generator.state_dict(), f\"saved_model_dcgan_flower2_256_{epoch}.pth\")\n",
    "        d_loss_values.append(d_loss.item())\n",
    "        g_loss_values.append(g_loss.item())\n",
    "    epoch_end_time = time.time()  # End time for the epoch\n",
    "    epoch_duration = epoch_end_time - epoch_start_time  # Duration of the epoch\n",
    "    epoch_times.append(epoch_duration)  # Append the duration to the list\n",
    "# Save model after last iter\n",
    "save_image(gen_imgs.data[:25], f\"images/{epoch}_DCGAN_flowers2_256.png\", nrow=5, normalize=True)\n",
    "# Save the model\n",
    "torch.save(generator.state_dict(), f\"saved_model_dcgan_flower2_256_{EPOCHS}.pth\")\n",
    "# Calculate average time per epoch\n",
    "average_time_per_epoch = sum(epoch_times) / len(epoch_times)\n",
    "print(f\"Average time per epoch: {average_time_per_epoch:.2f} seconds\")\n",
    "plt.plot(np.arange(0, EPOCHS,2), d_loss_values, label='Discriminator loss')\n",
    "plt.plot(np.arange(0, EPOCHS,2), g_loss_values, label='Generator loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss values')\n",
    "plt.savefig('loss_values_flowers_256.png')\n",
    "plt.show()\n",
    "# Create plot for time taken per epoch\n",
    "plt.plot(np.arange(0, EPOCHS), epoch_times)\n",
    "plt.title(\"Time taken per epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Time (s)\")\n",
    "plt.savefig(\"time_per_epoch_flowers_256.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
