digraph {
	graph [size="14.25,14.25"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2542040715888 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	2540024468624 [label=SigmoidBackward0]
	2540024468000 -> 2540024468624
	2540024468000 [label=AddmmBackward0]
	2536154778208 -> 2540024468000
	2542035132560 [label="model.12.bias
 (1)" fillcolor=lightblue]
	2542035132560 -> 2536154778208
	2536154778208 [label=AccumulateGrad]
	2536154778592 -> 2540024468000
	2536154778592 [label=ViewBackward0]
	2536154777968 -> 2536154778592
	2536154777968 [label=LeakyReluBackward1]
	2536154777920 -> 2536154777968
	2536154777920 [label=CudnnBatchNormBackward0]
	2536154777392 -> 2536154777920
	2536154777392 [label=ConvolutionBackward0]
	2536154779360 -> 2536154777392
	2536154779360 [label=LeakyReluBackward1]
	2536154778304 -> 2536154779360
	2536154778304 [label=CudnnBatchNormBackward0]
	2536154778880 -> 2536154778304
	2536154778880 [label=ConvolutionBackward0]
	2536154779216 -> 2536154778880
	2536154779216 [label=LeakyReluBackward1]
	2536154779168 -> 2536154779216
	2536154779168 [label=CudnnBatchNormBackward0]
	2536154779072 -> 2536154779168
	2536154779072 [label=ConvolutionBackward0]
	2536120515264 -> 2536154779072
	2536120515264 [label=LeakyReluBackward1]
	2541958377232 -> 2536120515264
	2541958377232 [label=ConvolutionBackward0]
	2542027903184 -> 2541958377232
	2541966680688 [label="model.0.weight
 (64, 3, 4, 4)" fillcolor=lightblue]
	2541966680688 -> 2542027903184
	2542027903184 [label=AccumulateGrad]
	2542044370208 -> 2541958377232
	2541966682208 [label="model.0.bias
 (64)" fillcolor=lightblue]
	2541966682208 -> 2542044370208
	2542044370208 [label=AccumulateGrad]
	2536154778256 -> 2536154779072
	2541966683088 [label="model.2.weight
 (128, 64, 4, 4)" fillcolor=lightblue]
	2541966683088 -> 2536154778256
	2536154778256 [label=AccumulateGrad]
	2536154778976 -> 2536154779072
	2541966683568 [label="model.2.bias
 (128)" fillcolor=lightblue]
	2541966683568 -> 2536154778976
	2536154778976 [label=AccumulateGrad]
	2536154779120 -> 2536154779168
	2541966683888 [label="model.3.weight
 (128)" fillcolor=lightblue]
	2541966683888 -> 2536154779120
	2536154779120 [label=AccumulateGrad]
	2536154777824 -> 2536154779168
	2541966683968 [label="model.3.bias
 (128)" fillcolor=lightblue]
	2541966683968 -> 2536154777824
	2536154777824 [label=AccumulateGrad]
	2536154779600 -> 2536154778880
	2541966684048 [label="model.5.weight
 (256, 128, 4, 4)" fillcolor=lightblue]
	2541966684048 -> 2536154779600
	2536154779600 [label=AccumulateGrad]
	2536154778496 -> 2536154778880
	2541966680208 [label="model.5.bias
 (256)" fillcolor=lightblue]
	2541966680208 -> 2536154778496
	2536154778496 [label=AccumulateGrad]
	2536154778928 -> 2536154778304
	2541966681088 [label="model.6.weight
 (256)" fillcolor=lightblue]
	2541966681088 -> 2536154778928
	2536154778928 [label=AccumulateGrad]
	2536154779504 -> 2536154778304
	2541966680128 [label="model.6.bias
 (256)" fillcolor=lightblue]
	2541966680128 -> 2536154779504
	2536154779504 [label=AccumulateGrad]
	2536154777248 -> 2536154777392
	2542035132800 [label="model.8.weight
 (512, 256, 4, 4)" fillcolor=lightblue]
	2542035132800 -> 2536154777248
	2536154777248 [label=AccumulateGrad]
	2536154778064 -> 2536154777392
	2542035133440 [label="model.8.bias
 (512)" fillcolor=lightblue]
	2542035133440 -> 2536154778064
	2536154778064 [label=AccumulateGrad]
	2536154777536 -> 2536154777920
	2542035132720 [label="model.9.weight
 (512)" fillcolor=lightblue]
	2542035132720 -> 2536154777536
	2536154777536 [label=AccumulateGrad]
	2536154778736 -> 2536154777920
	2542035133520 [label="model.9.bias
 (512)" fillcolor=lightblue]
	2542035133520 -> 2536154778736
	2536154778736 [label=AccumulateGrad]
	2536154777200 -> 2540024468000
	2536154777200 [label=TBackward0]
	2536154777872 -> 2536154777200
	2542035134080 [label="model.12.weight
 (1, 32768)" fillcolor=lightblue]
	2542035134080 -> 2536154777872
	2536154777872 [label=AccumulateGrad]
	2540024468624 -> 2542040715888
}
