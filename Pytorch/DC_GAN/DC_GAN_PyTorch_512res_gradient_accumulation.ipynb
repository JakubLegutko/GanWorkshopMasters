{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Moved to 512x512 as the final step. Batch size had to drop to 8 here, but employed gradient accumulation to emulate a bigger batch size. This however didn't improve the results. I can see that gradient accumulation doesn't work exactly the same way as increasing the batch size directly.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from torchviz import make_dot\n",
    "from torchinfo import summary\n",
    "\n",
    "d_loss_values = []\n",
    "g_loss_values = []\n",
    "epoch_times = []  # List to store time taken for each epoch\n",
    "\n",
    "# Define constants\n",
    "IMG_SIZE = 512  # Set to 512x512 for the new output size\n",
    "LATENT_DIM = 200  # Updated latent dimension\n",
    "BATCH_SIZE = 8  # Updated batch size\n",
    "EPOCHS = 100\n",
    "accum_steps = 12  # Number of accumulation steps to simulate a larger batch size\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.init_size = IMG_SIZE // 32  # Adjusted for 512x512 images (512 / 32 = 16)\n",
    "        self.l1 = nn.Sequential(nn.Linear(LATENT_DIM, 512 * self.init_size ** 2))\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(512, 256, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256, 0.8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(256, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(64, 32, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32, 0.8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(32, 16, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16, 0.8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 3, 3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.l1(z)\n",
    "        out = out.view(out.shape[0], 512, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, stride=2, padding=1),   # 512x512 -> 256x256\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 256x256 -> 128x128\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, 4, stride=2, padding=1), # 128x128 -> 64x64\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, 4, stride=2, padding=1), # 64x64 -> 32x32\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, 1024, 4, stride=2, padding=1), # 32x32 -> 16x16\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(1024, 2048, 4, stride=2, padding=1), # 16x16 -> 8x8\n",
    "            nn.BatchNorm2d(2048),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2048 * 8 * 8, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        validity = self.model(img)\n",
    "        return validity\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "dataloader = DataLoader(\n",
    "    ConcatDataset([datasets.Flowers102(root='../../data/flowers', split='train', download=True, transform=transform),\n",
    "                   datasets.Flowers102(root='../../data/flowers', split='val', download=True, transform=transform),\n",
    "                   datasets.Flowers102(root='../../data/flowers', split='test', download=True, transform=transform)]),\n",
    "    batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "\n",
    "# Initialize models\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "# Apply weights initialization to models\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n",
    "\n",
    "# Loss function\n",
    "adversarial_loss = nn.BCELoss()\n",
    "\n",
    "# Optimizers (consider lowering the learning rate slightly)\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0003, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_start_time = time.time()\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "        real_imgs = imgs.to(device)\n",
    "        batch_size = real_imgs.size(0)\n",
    "\n",
    "        # Labels\n",
    "        valid = torch.ones(batch_size, 1, requires_grad=False).to(device)\n",
    "        fake = torch.zeros(batch_size, 1, requires_grad=False).to(device)\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        optimizer_D.zero_grad(set_to_none=True)\n",
    "\n",
    "        z = torch.randn(batch_size, LATENT_DIM).to(device)\n",
    "        gen_imgs = generator(z)\n",
    "\n",
    "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
    "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        # Accumulate gradients\n",
    "        d_loss = d_loss / accum_steps\n",
    "        d_loss.backward()\n",
    "\n",
    "        if (i + 1) % accum_steps == 0 or (i + 1) == len(dataloader):\n",
    "            optimizer_D.step()\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "        optimizer_G.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Measure generator's ability to fool the discriminator\n",
    "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
    "        g_loss = g_loss / accum_steps\n",
    "        g_loss.backward()\n",
    "\n",
    "        if (i + 1) % accum_steps == 0 or (i + 1) == len(dataloader):\n",
    "            optimizer_G.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch}/{EPOCHS}] [Batch {i}/{len(dataloader)}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}]\")\n",
    "        print(f\"[GPU Memory Allocated: {allocated_memory:.2f} GB] [GPU Memory Reserved: {reserved_memory:.2f} GB]\")\n",
    "\n",
    "    # Save sample images and model checkpoints every few epochs\n",
    "    if epoch % 2 == 0:\n",
    "        save_image(gen_imgs.data[:25], f\"images/{epoch}_DCGAN_Flowers_bigbatch_R1_512.png\", nrow=5, normalize=True)\n",
    "        torch.save(generator.state_dict(), f\"saved_model/saved_model_dcgan_Flowers_bigbatch_R1_512_{epoch}.pth\")\n",
    "        d_loss_values.append(d_loss.item())\n",
    "        g_loss_values.append(g_loss.item())\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_times.append(epoch_end_time - epoch_start_time)\n",
    "\n",
    "# Save model and plot training progress\n",
    "torch.save(generator.state_dict(), f\"saved_model/saved_model_dcgan_Flowers_bigbatch_R1_512_{EPOCHS}.pth\")\n",
    "\n",
    "average_time_per_epoch = sum(epoch_times) / len(epoch_times)\n",
    "print(f\"Average time per epoch: {average_time_per_epoch:.2f} seconds\")\n",
    "\n",
    "plt.plot(np.arange(0, EPOCHS, 2), d_loss_values, label='Discriminator loss')\n",
    "plt.plot(np.arange(0, EPOCHS, 2), g_loss_values, label='Generator loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss values')\n",
    "plt.savefig('loss_values_flowers_512.png')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(np.arange(0, EPOCHS), epoch_times)\n",
    "plt.title(\"Time taken per epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Time (s)\")\n",
    "plt.savefig(\"time_per_epoch_flowers_512.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tried to emulate even bigger batch size in order to see some better results - the pictures are better, but not by much - the model's architecture might be the bottleneck here more than the batch size.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from torchviz import make_dot\n",
    "from torchinfo import summary\n",
    "\n",
    "d_loss_values = []\n",
    "g_loss_values = []\n",
    "epoch_times = []  # List to store time taken for each epoch\n",
    "\n",
    "# Define constants\n",
    "IMG_SIZE = 512  # Set to 512x512 for the new output size\n",
    "LATENT_DIM = 200  # Updated latent dimension\n",
    "BATCH_SIZE = 8  # Updated batch size\n",
    "EPOCHS = 100\n",
    "accum_steps = 36  # Number of accumulation steps to simulate a larger batch size\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.init_size = IMG_SIZE // 32  # Adjusted for 512x512 images (512 / 32 = 16)\n",
    "        self.l1 = nn.Sequential(nn.Linear(LATENT_DIM, 512 * self.init_size ** 2))\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(512, 256, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256, 0.8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(256, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(64, 32, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32, 0.8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(32, 16, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16, 0.8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 3, 3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.l1(z)\n",
    "        out = out.view(out.shape[0], 512, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, stride=2, padding=1),   # 512x512 -> 256x256\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 256x256 -> 128x128\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, 4, stride=2, padding=1), # 128x128 -> 64x64\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, 4, stride=2, padding=1), # 64x64 -> 32x32\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, 1024, 4, stride=2, padding=1), # 32x32 -> 16x16\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(1024, 2048, 4, stride=2, padding=1), # 16x16 -> 8x8\n",
    "            nn.BatchNorm2d(2048),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2048 * 8 * 8, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        validity = self.model(img)\n",
    "        return validity\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "dataloader = DataLoader(\n",
    "    ConcatDataset([datasets.Flowers102(root='../../data/flowers', split='train', download=True, transform=transform),\n",
    "                   datasets.Flowers102(root='../../data/flowers', split='val', download=True, transform=transform),\n",
    "                   datasets.Flowers102(root='../../data/flowers', split='test', download=True, transform=transform)]),\n",
    "    batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "\n",
    "# Initialize models\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "# Apply weights initialization to models\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n",
    "\n",
    "# Loss function\n",
    "adversarial_loss = nn.BCELoss()\n",
    "\n",
    "# Optimizers (consider lowering the learning rate slightly)\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0003, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_start_time = time.time()\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "        real_imgs = imgs.to(device)\n",
    "        batch_size = real_imgs.size(0)\n",
    "\n",
    "        # Labels\n",
    "        valid = torch.ones(batch_size, 1, requires_grad=False).to(device)\n",
    "        fake = torch.zeros(batch_size, 1, requires_grad=False).to(device)\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        optimizer_D.zero_grad(set_to_none=True)\n",
    "\n",
    "        z = torch.randn(batch_size, LATENT_DIM).to(device)\n",
    "        gen_imgs = generator(z)\n",
    "\n",
    "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
    "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        # Accumulate gradients\n",
    "        d_loss = d_loss / accum_steps\n",
    "        d_loss.backward()\n",
    "\n",
    "        if (i + 1) % accum_steps == 0 or (i + 1) == len(dataloader):\n",
    "            optimizer_D.step()\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "        optimizer_G.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Measure generator's ability to fool the discriminator\n",
    "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
    "        g_loss = g_loss / accum_steps\n",
    "        g_loss.backward()\n",
    "\n",
    "        if (i + 1) % accum_steps == 0 or (i + 1) == len(dataloader):\n",
    "            optimizer_G.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch}/{EPOCHS}] [Batch {i}/{len(dataloader)}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}]\")\n",
    "        print(f\"[GPU Memory Allocated: {allocated_memory:.2f} GB] [GPU Memory Reserved: {reserved_memory:.2f} GB]\")\n",
    "\n",
    "    # Save sample images and model checkpoints every few epochs\n",
    "    if epoch % 2 == 0:\n",
    "        save_image(gen_imgs.data[:25], f\"images/{epoch}_DCGAN_Flowers_gigabatch_R1_512.png\", nrow=5, normalize=True)\n",
    "        torch.save(generator.state_dict(), f\"saved_model/saved_model_dcgan_Flowers_gigabatch_R1_512_{epoch}.pth\")\n",
    "        d_loss_values.append(d_loss.item())\n",
    "        g_loss_values.append(g_loss.item())\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_times.append(epoch_end_time - epoch_start_time)\n",
    "\n",
    "# Save model and plot training progress\n",
    "torch.save(generator.state_dict(), f\"saved_model/saved_model_dcgan_Flowers_gigabatch_R1_512_{EPOCHS}.pth\")\n",
    "\n",
    "average_time_per_epoch = sum(epoch_times) / len(epoch_times)\n",
    "print(f\"Average time per epoch: {average_time_per_epoch:.2f} seconds\")\n",
    "\n",
    "plt.plot(np.arange(0, EPOCHS, 2), d_loss_values, label='Discriminator loss')\n",
    "plt.plot(np.arange(0, EPOCHS, 2), g_loss_values, label='Generator loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss values')\n",
    "plt.savefig('loss_values_flowers_512_gigabatch.png')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(np.arange(0, EPOCHS), epoch_times)\n",
    "plt.title(\"Time taken per epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Time (s)\")\n",
    "plt.savefig(\"time_per_epoch_flowers_512_gigabatch.png\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
