{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1711.11586"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Models\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "##############################\n",
    "#           U-NET\n",
    "##############################\n",
    "\n",
    "\n",
    "class UNetDown(nn.Module):\n",
    "    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n",
    "        super(UNetDown, self).__init__()\n",
    "        layers = [nn.Conv2d(in_size, out_size, 3, stride=2, padding=1, bias=False)]\n",
    "        if normalize:\n",
    "            layers.append(nn.BatchNorm2d(out_size, 0.8))\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class UNetUp(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(UNetUp, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(in_size, out_size, 3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_size, 0.8),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip_input):\n",
    "        x = self.model(x)\n",
    "        x = torch.cat((x, skip_input), 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, img_shape):\n",
    "        super(Generator, self).__init__()\n",
    "        channels, self.h, self.w = img_shape\n",
    "\n",
    "        self.fc = nn.Linear(latent_dim, self.h * self.w)\n",
    "\n",
    "        self.down1 = UNetDown(channels + 1, 64, normalize=False)\n",
    "        self.down2 = UNetDown(64, 128)\n",
    "        self.down3 = UNetDown(128, 256)\n",
    "        self.down4 = UNetDown(256, 512)\n",
    "        self.down5 = UNetDown(512, 512)\n",
    "        self.down6 = UNetDown(512, 512)\n",
    "        self.down7 = UNetDown(512, 512, normalize=False)\n",
    "        self.up1 = UNetUp(512, 512)\n",
    "        self.up2 = UNetUp(1024, 512)\n",
    "        self.up3 = UNetUp(1024, 512)\n",
    "        self.up4 = UNetUp(1024, 256)\n",
    "        self.up5 = UNetUp(512, 128)\n",
    "        self.up6 = UNetUp(256, 64)\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2), nn.Conv2d(128, channels, 3, stride=1, padding=1), nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        # Propogate noise through fc layer and reshape to img shape\n",
    "        z = self.fc(z).view(z.size(0), 1, self.h, self.w)\n",
    "        d1 = self.down1(torch.cat((x, z), 1))\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        d6 = self.down6(d5)\n",
    "        d7 = self.down7(d6)\n",
    "        u1 = self.up1(d7, d6)\n",
    "        u2 = self.up2(u1, d5)\n",
    "        u3 = self.up3(u2, d4)\n",
    "        u4 = self.up4(u3, d3)\n",
    "        u5 = self.up5(u4, d2)\n",
    "        u6 = self.up6(u5, d1)\n",
    "\n",
    "        return self.final(u6)\n",
    "\n",
    "\n",
    "##############################\n",
    "#        Encoder\n",
    "##############################\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim, input_shape):\n",
    "        super(Encoder, self).__init__()\n",
    "        resnet18_model = resnet18(pretrained=False)\n",
    "        self.feature_extractor = nn.Sequential(*list(resnet18_model.children())[:-3])\n",
    "        self.pooling = nn.AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
    "        # Output is mu and log(var) for reparameterization trick used in VAEs\n",
    "        self.fc_mu = nn.Linear(256, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256, latent_dim)\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.feature_extractor(img)\n",
    "        out = self.pooling(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        mu = self.fc_mu(out)\n",
    "        logvar = self.fc_logvar(out)\n",
    "        return mu, logvar\n",
    "\n",
    "\n",
    "##############################\n",
    "#        Discriminator\n",
    "##############################\n",
    "\n",
    "\n",
    "class MultiDiscriminator(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(MultiDiscriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, normalize=True):\n",
    "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2))\n",
    "            return layers\n",
    "\n",
    "        channels, _, _ = input_shape\n",
    "        # Extracts discriminator models\n",
    "        self.models = nn.ModuleList()\n",
    "        for i in range(3):\n",
    "            self.models.add_module(\n",
    "                \"disc_%d\" % i,\n",
    "                nn.Sequential(\n",
    "                    *discriminator_block(channels, 64, normalize=False),\n",
    "                    *discriminator_block(64, 128),\n",
    "                    *discriminator_block(128, 256),\n",
    "                    *discriminator_block(256, 512),\n",
    "                    nn.Conv2d(512, 1, 3, padding=1)\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        self.downsample = nn.AvgPool2d(channels, stride=2, padding=[1, 1], count_include_pad=False)\n",
    "\n",
    "    def compute_loss(self, x, gt):\n",
    "        \"\"\"Computes the MSE between model output and scalar gt\"\"\"\n",
    "        loss = sum([torch.mean((out - gt) ** 2) for out in self.forward(x)])\n",
    "        return loss\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for m in self.models:\n",
    "            outputs.append(m(x))\n",
    "            x = self.downsample(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "import glob\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, input_shape, mode=\"train\"):\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(input_shape[-2:], Image.BICUBIC),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.files = sorted(glob.glob(os.path.join(root, mode) + \"/*.*\"))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        img = Image.open(self.files[index % len(self.files)])\n",
    "        w, h = img.size\n",
    "        img_A = img.crop((0, 0, w / 2, h))\n",
    "        img_B = img.crop((w / 2, 0, w, h))\n",
    "\n",
    "        if np.random.random() < 0.5:\n",
    "            img_A = Image.fromarray(np.array(img_A)[:, ::-1, :], \"RGB\")\n",
    "            img_B = Image.fromarray(np.array(img_B)[:, ::-1, :], \"RGB\")\n",
    "\n",
    "        img_A = self.transform(img_A)\n",
    "        img_B = self.transform(img_B)\n",
    "\n",
    "        return {\"A\": img_A, \"B\": img_B}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Args object at 0x000001B244217B80>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ZULUL\\Desktop\\ML\\.venv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ZULUL\\Desktop\\ML\\.venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'in_channels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 60\u001b[0m\n\u001b[0;32m     58\u001b[0m generator \u001b[38;5;241m=\u001b[39m Generator(opt\u001b[38;5;241m.\u001b[39mlatent_dim, input_shape)\n\u001b[0;32m     59\u001b[0m encoder \u001b[38;5;241m=\u001b[39m Encoder(opt\u001b[38;5;241m.\u001b[39mlatent_dim, input_shape)\n\u001b[1;32m---> 60\u001b[0m D_VAE \u001b[38;5;241m=\u001b[39m \u001b[43mMultiDiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m D_LR \u001b[38;5;241m=\u001b[39m MultiDiscriminator(input_shape)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cuda:\n",
      "File \u001b[1;32mc:\\Users\\ZULUL\\DLML\\Gan_workshop_masters\\Pytorch\\Bicycle_GAN\\models.py:154\u001b[0m, in \u001b[0;36mMultiDiscriminator.__init__\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39madd_module(\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisc_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m i,\n\u001b[0;32m    145\u001b[0m         nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    151\u001b[0m         ),\n\u001b[0;32m    152\u001b[0m     )\n\u001b[1;32m--> 154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mAvgPool2d(\u001b[43min_channels\u001b[49m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, padding\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m], count_include_pad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'in_channels' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "import datetime\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "\n",
    "# Create the args class, fill with default values instead of using the parser\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        self.n_epochs = 200\n",
    "        self.dataset_name = \"edges2shoes\"\n",
    "        self.batch_size = 8\n",
    "        self.lr = 0.0002\n",
    "        self.b1 = 0.5\n",
    "        self.b2 = 0.999\n",
    "        self.n_cpu = 8\n",
    "        self.img_height = 128\n",
    "        self.img_width = 128\n",
    "        self.channels = 3\n",
    "        self.latent_dim = 8\n",
    "        self.sample_interval = 400\n",
    "        self.checkpoint_interval = -1\n",
    "        self.lambda_pixel = 10\n",
    "        self.lambda_latent = 0.5\n",
    "        self.lambda_kl = 0.01\n",
    "opt = Args()\n",
    "print(opt)\n",
    "\n",
    "os.makedirs(\"images/%s\" % opt.dataset_name, exist_ok=True)\n",
    "os.makedirs(\"saved_models/%s\" % opt.dataset_name, exist_ok=True)\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "input_shape = (opt.channels, opt.img_height, opt.img_width)\n",
    "\n",
    "# Loss functions\n",
    "mae_loss = torch.nn.L1Loss()\n",
    "\n",
    "# Initialize generator, encoder and discriminators\n",
    "generator = Generator(opt.latent_dim, input_shape)\n",
    "encoder = Encoder(opt.latent_dim, input_shape)\n",
    "D_VAE = MultiDiscriminator(input_shape)\n",
    "D_LR = MultiDiscriminator(input_shape)\n",
    "\n",
    "if cuda:\n",
    "    generator = generator.cuda()\n",
    "    encoder.cuda()\n",
    "    D_VAE = D_VAE.cuda()\n",
    "    D_LR = D_LR.cuda()\n",
    "    mae_loss.cuda()\n",
    "\n",
    "if opt.epoch != 0:\n",
    "    # Load pretrained models\n",
    "    generator.load_state_dict(torch.load(\"saved_models/%s/generator_%d.pth\" % (opt.dataset_name, opt.epoch)))\n",
    "    encoder.load_state_dict(torch.load(\"saved_models/%s/encoder_%d.pth\" % (opt.dataset_name, opt.epoch)))\n",
    "    D_VAE.load_state_dict(torch.load(\"saved_models/%s/D_VAE_%d.pth\" % (opt.dataset_name, opt.epoch)))\n",
    "    D_LR.load_state_dict(torch.load(\"saved_models/%s/D_LR_%d.pth\" % (opt.dataset_name, opt.epoch)))\n",
    "else:\n",
    "    # Initialize weights\n",
    "    generator.apply(weights_init_normal)\n",
    "    D_VAE.apply(weights_init_normal)\n",
    "    D_LR.apply(weights_init_normal)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_E = torch.optim.Adam(encoder.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "optimizer_D_VAE = torch.optim.Adam(D_VAE.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "optimizer_D_LR = torch.optim.Adam(D_LR.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    ImageDataset(\"../../data/%s\" % opt.dataset_name, input_shape),\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=opt.n_cpu,\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    ImageDataset(\"../../data/%s\" % opt.dataset_name, input_shape, mode=\"val\"),\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    ")\n",
    "\n",
    "\n",
    "def sample_images(batches_done):\n",
    "    \"\"\"Saves a generated sample from the validation set\"\"\"\n",
    "    generator.eval()\n",
    "    imgs = next(iter(val_dataloader))\n",
    "    img_samples = None\n",
    "    for img_A, img_B in zip(imgs[\"A\"], imgs[\"B\"]):\n",
    "        # Repeat input image by number of desired columns\n",
    "        real_A = img_A.view(1, *img_A.shape).repeat(opt.latent_dim, 1, 1, 1)\n",
    "        real_A = Variable(real_A.type(Tensor))\n",
    "        # Sample latent representations\n",
    "        sampled_z = Variable(Tensor(np.random.normal(0, 1, (opt.latent_dim, opt.latent_dim))))\n",
    "        # Generate samples\n",
    "        fake_B = generator(real_A, sampled_z)\n",
    "        # Concatenate samples horisontally\n",
    "        fake_B = torch.cat([x for x in fake_B.data.cpu()], -1)\n",
    "        img_sample = torch.cat((img_A, fake_B), -1)\n",
    "        img_sample = img_sample.view(1, *img_sample.shape)\n",
    "        # Concatenate with previous samples vertically\n",
    "        img_samples = img_sample if img_samples is None else torch.cat((img_samples, img_sample), -2)\n",
    "    save_image(img_samples, \"images/%s/%s.png\" % (opt.dataset_name, batches_done), nrow=8, normalize=True)\n",
    "    generator.train()\n",
    "\n",
    "\n",
    "def reparameterization(mu, logvar):\n",
    "    std = torch.exp(logvar / 2)\n",
    "    sampled_z = Variable(Tensor(np.random.normal(0, 1, (mu.size(0), opt.latent_dim))))\n",
    "    z = sampled_z * std + mu\n",
    "    return z\n",
    "\n",
    "\n",
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "\n",
    "# Adversarial loss\n",
    "valid = 1\n",
    "fake = 0\n",
    "\n",
    "prev_time = time.time()\n",
    "for epoch in range(opt.epoch, opt.n_epochs):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "\n",
    "        # Set model input\n",
    "        real_A = Variable(batch[\"A\"].type(Tensor))\n",
    "        real_B = Variable(batch[\"B\"].type(Tensor))\n",
    "\n",
    "        # -------------------------------\n",
    "        #  Train Generator and Encoder\n",
    "        # -------------------------------\n",
    "\n",
    "        optimizer_E.zero_grad()\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # ----------\n",
    "        # cVAE-GAN\n",
    "        # ----------\n",
    "\n",
    "        # Produce output using encoding of B (cVAE-GAN)\n",
    "        mu, logvar = encoder(real_B)\n",
    "        encoded_z = reparameterization(mu, logvar)\n",
    "        fake_B = generator(real_A, encoded_z)\n",
    "\n",
    "        # Pixelwise loss of translated image by VAE\n",
    "        loss_pixel = mae_loss(fake_B, real_B)\n",
    "        # Kullback-Leibler divergence of encoded B\n",
    "        loss_kl = 0.5 * torch.sum(torch.exp(logvar) + mu ** 2 - logvar - 1)\n",
    "        # Adversarial loss\n",
    "        loss_VAE_GAN = D_VAE.compute_loss(fake_B, valid)\n",
    "\n",
    "        # ---------\n",
    "        # cLR-GAN\n",
    "        # ---------\n",
    "\n",
    "        # Produce output using sampled z (cLR-GAN)\n",
    "        sampled_z = Variable(Tensor(np.random.normal(0, 1, (real_A.size(0), opt.latent_dim))))\n",
    "        _fake_B = generator(real_A, sampled_z)\n",
    "        # cLR Loss: Adversarial loss\n",
    "        loss_LR_GAN = D_LR.compute_loss(_fake_B, valid)\n",
    "\n",
    "        # ----------------------------------\n",
    "        # Total Loss (Generator + Encoder)\n",
    "        # ----------------------------------\n",
    "\n",
    "        loss_GE = loss_VAE_GAN + loss_LR_GAN + opt.lambda_pixel * loss_pixel + opt.lambda_kl * loss_kl\n",
    "\n",
    "        loss_GE.backward(retain_graph=True)\n",
    "        optimizer_E.step()\n",
    "\n",
    "        # ---------------------\n",
    "        # Generator Only Loss\n",
    "        # ---------------------\n",
    "\n",
    "        # Latent L1 loss\n",
    "        _mu, _ = encoder(_fake_B)\n",
    "        loss_latent = opt.lambda_latent * mae_loss(_mu, sampled_z)\n",
    "\n",
    "        loss_latent.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ----------------------------------\n",
    "        #  Train Discriminator (cVAE-GAN)\n",
    "        # ----------------------------------\n",
    "\n",
    "        optimizer_D_VAE.zero_grad()\n",
    "\n",
    "        loss_D_VAE = D_VAE.compute_loss(real_B, valid) + D_VAE.compute_loss(fake_B.detach(), fake)\n",
    "\n",
    "        loss_D_VAE.backward()\n",
    "        optimizer_D_VAE.step()\n",
    "\n",
    "        # ---------------------------------\n",
    "        #  Train Discriminator (cLR-GAN)\n",
    "        # ---------------------------------\n",
    "\n",
    "        optimizer_D_LR.zero_grad()\n",
    "\n",
    "        loss_D_LR = D_LR.compute_loss(real_B, valid) + D_LR.compute_loss(_fake_B.detach(), fake)\n",
    "\n",
    "        loss_D_LR.backward()\n",
    "        optimizer_D_LR.step()\n",
    "\n",
    "        # --------------\n",
    "        #  Log Progress\n",
    "        # --------------\n",
    "\n",
    "        # Determine approximate time left\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        batches_left = opt.n_epochs * len(dataloader) - batches_done\n",
    "        time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
    "        prev_time = time.time()\n",
    "\n",
    "        # Print log\n",
    "        sys.stdout.write(\n",
    "            \"\\r[Epoch %d/%d] [Batch %d/%d] [D VAE_loss: %f, LR_loss: %f] [G loss: %f, pixel: %f, kl: %f, latent: %f] ETA: %s\"\n",
    "            % (\n",
    "                epoch,\n",
    "                opt.n_epochs,\n",
    "                i,\n",
    "                len(dataloader),\n",
    "                loss_D_VAE.item(),\n",
    "                loss_D_LR.item(),\n",
    "                loss_GE.item(),\n",
    "                loss_pixel.item(),\n",
    "                loss_kl.item(),\n",
    "                loss_latent.item(),\n",
    "                time_left,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if batches_done % opt.sample_interval == 0:\n",
    "            sample_images(batches_done)\n",
    "\n",
    "    if opt.checkpoint_interval != -1 and epoch % opt.checkpoint_interval == 0:\n",
    "        # Save model checkpoints\n",
    "        torch.save(generator.state_dict(), \"saved_models/%s/generator_%d.pth\" % (opt.dataset_name, epoch))\n",
    "        torch.save(encoder.state_dict(), \"saved_models/%s/encoder_%d.pth\" % (opt.dataset_name, epoch))\n",
    "        torch.save(D_VAE.state_dict(), \"saved_models/%s/D_VAE_%d.pth\" % (opt.dataset_name, epoch))\n",
    "        torch.save(D_LR.state_dict(), \"saved_models/%s/D_LR_%d.pth\" % (opt.dataset_name, epoch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
