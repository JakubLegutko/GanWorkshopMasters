{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 1/100] [Batch 0/782] [D loss: -3.5688352584838867] [G loss: 4.271856307983398]\n",
      "[Epoch 1/100] [Batch 100/782] [D loss: -575.4093627929688] [G loss: 282.6064453125]\n",
      "[Epoch 1/100] [Batch 200/782] [D loss: 19.078365325927734] [G loss: -456.93023681640625]\n",
      "[Epoch 1/100] [Batch 300/782] [D loss: -10.034337997436523] [G loss: -462.74908447265625]\n",
      "[Epoch 1/100] [Batch 400/782] [D loss: -16.070066452026367] [G loss: -433.5330810546875]\n",
      "[Epoch 1/100] [Batch 500/782] [D loss: -101.57219696044922] [G loss: -313.2353210449219]\n",
      "[Epoch 1/100] [Batch 600/782] [D loss: -620.7971801757812] [G loss: 137.49774169921875]\n",
      "[Epoch 1/100] [Batch 700/782] [D loss: -23.38006591796875] [G loss: -465.18389892578125]\n",
      "[Epoch 2/100] [Batch 0/782] [D loss: -105.55667114257812] [G loss: -395.8480224609375]\n",
      "[Epoch 2/100] [Batch 100/782] [D loss: -7.579474449157715] [G loss: -439.79876708984375]\n",
      "[Epoch 2/100] [Batch 200/782] [D loss: -14.512287139892578] [G loss: -419.3524169921875]\n",
      "[Epoch 2/100] [Batch 300/782] [D loss: -66.0875473022461] [G loss: -391.43524169921875]\n",
      "[Epoch 2/100] [Batch 400/782] [D loss: -383.3560485839844] [G loss: -135.5738983154297]\n",
      "[Epoch 2/100] [Batch 500/782] [D loss: -176.5763397216797] [G loss: -204.35855102539062]\n",
      "[Epoch 2/100] [Batch 600/782] [D loss: -1289.320068359375] [G loss: 601.0782470703125]\n",
      "[Epoch 2/100] [Batch 700/782] [D loss: -2476.66064453125] [G loss: 1407.6754150390625]\n",
      "[Epoch 3/100] [Batch 0/782] [D loss: -208.16831970214844] [G loss: -1516.1358642578125]\n",
      "[Epoch 3/100] [Batch 100/782] [D loss: -5362.2392578125] [G loss: 2874.76611328125]\n",
      "[Epoch 3/100] [Batch 200/782] [D loss: -5752.67431640625] [G loss: 3306.85888671875]\n",
      "[Epoch 3/100] [Batch 300/782] [D loss: -7577.7294921875] [G loss: 3836.519287109375]\n",
      "[Epoch 3/100] [Batch 400/782] [D loss: -7700.033203125] [G loss: 4252.291015625]\n",
      "[Epoch 3/100] [Batch 500/782] [D loss: -23.590087890625] [G loss: -2173.02490234375]\n",
      "[Epoch 3/100] [Batch 600/782] [D loss: -137.68966674804688] [G loss: -2734.076416015625]\n",
      "[Epoch 3/100] [Batch 700/782] [D loss: -8370.103515625] [G loss: 4870.7216796875]\n",
      "[Epoch 4/100] [Batch 0/782] [D loss: -11607.296875] [G loss: 6043.44287109375]\n",
      "[Epoch 4/100] [Batch 100/782] [D loss: -13774.03125] [G loss: 7081.68017578125]\n",
      "[Epoch 4/100] [Batch 200/782] [D loss: -13816.6220703125] [G loss: 7467.4072265625]\n",
      "[Epoch 4/100] [Batch 300/782] [D loss: -15806.265625] [G loss: 8284.13671875]\n",
      "[Epoch 4/100] [Batch 400/782] [D loss: -16427.814453125] [G loss: 7680.35302734375]\n",
      "[Epoch 4/100] [Batch 500/782] [D loss: -12303.439453125] [G loss: 6193.65234375]\n",
      "[Epoch 4/100] [Batch 600/782] [D loss: -19857.134765625] [G loss: 10911.28515625]\n",
      "[Epoch 4/100] [Batch 700/782] [D loss: -19518.716796875] [G loss: 10210.626953125]\n",
      "[Epoch 5/100] [Batch 0/782] [D loss: -10042.548828125] [G loss: 9760.72265625]\n",
      "[Epoch 5/100] [Batch 100/782] [D loss: -21399.73828125] [G loss: 12172.7744140625]\n",
      "[Epoch 5/100] [Batch 200/782] [D loss: -26604.619140625] [G loss: 13898.91796875]\n",
      "[Epoch 5/100] [Batch 300/782] [D loss: -28530.419921875] [G loss: 15427.953125]\n",
      "[Epoch 5/100] [Batch 400/782] [D loss: -25389.994140625] [G loss: 12909.0048828125]\n",
      "[Epoch 5/100] [Batch 500/782] [D loss: -32206.189453125] [G loss: 17294.14453125]\n",
      "[Epoch 5/100] [Batch 600/782] [D loss: -34942.07421875] [G loss: 18137.5859375]\n",
      "[Epoch 5/100] [Batch 700/782] [D loss: -39726.62890625] [G loss: 20415.30859375]\n",
      "[Epoch 6/100] [Batch 0/782] [D loss: -40863.78515625] [G loss: 20963.68359375]\n",
      "[Epoch 6/100] [Batch 100/782] [D loss: -46481.171875] [G loss: 23566.431640625]\n",
      "[Epoch 6/100] [Batch 200/782] [D loss: -49763.25390625] [G loss: 25228.287109375]\n",
      "[Epoch 6/100] [Batch 300/782] [D loss: -52995.12109375] [G loss: 26858.625]\n",
      "[Epoch 6/100] [Batch 400/782] [D loss: -56328.1875] [G loss: 28469.22265625]\n",
      "[Epoch 6/100] [Batch 500/782] [D loss: -59902.1171875] [G loss: 30130.8515625]\n",
      "[Epoch 6/100] [Batch 600/782] [D loss: -1796.8271484375] [G loss: -22551.20703125]\n",
      "[Epoch 6/100] [Batch 700/782] [D loss: -63514.51171875] [G loss: 32055.95703125]\n",
      "[Epoch 7/100] [Batch 0/782] [D loss: 3859.77880859375] [G loss: 1069.961181640625]\n",
      "[Epoch 7/100] [Batch 100/782] [D loss: 87.78578186035156] [G loss: -608.1220703125]\n",
      "[Epoch 7/100] [Batch 200/782] [D loss: -234.7525634765625] [G loss: -984.68505859375]\n",
      "[Epoch 7/100] [Batch 300/782] [D loss: 38.66242980957031] [G loss: -85.62979125976562]\n",
      "[Epoch 7/100] [Batch 400/782] [D loss: -13289.25] [G loss: 24384.5]\n",
      "[Epoch 7/100] [Batch 500/782] [D loss: -70868.4765625] [G loss: 36137.625]\n",
      "[Epoch 7/100] [Batch 600/782] [D loss: -12087.8056640625] [G loss: 28800.37890625]\n",
      "[Epoch 7/100] [Batch 700/782] [D loss: -77051.8046875] [G loss: 39475.3359375]\n",
      "[Epoch 8/100] [Batch 0/782] [D loss: 3387.46484375] [G loss: 1376.35009765625]\n",
      "[Epoch 8/100] [Batch 100/782] [D loss: -419.39251708984375] [G loss: -584.9082641601562]\n",
      "[Epoch 8/100] [Batch 200/782] [D loss: -81248.5390625] [G loss: 42182.28515625]\n",
      "[Epoch 8/100] [Batch 300/782] [D loss: -88276.4375] [G loss: 44545.8125]\n",
      "[Epoch 8/100] [Batch 400/782] [D loss: -93037.3515625] [G loss: 47213.171875]\n",
      "[Epoch 8/100] [Batch 500/782] [D loss: -98075.78125] [G loss: 49530.9765625]\n",
      "[Epoch 8/100] [Batch 600/782] [D loss: -103357.6953125] [G loss: 52092.46875]\n",
      "[Epoch 8/100] [Batch 700/782] [D loss: -107698.484375] [G loss: 54248.62109375]\n",
      "[Epoch 9/100] [Batch 0/782] [D loss: -107199.15625] [G loss: 54350.3671875]\n",
      "[Epoch 9/100] [Batch 100/782] [D loss: -110804.265625] [G loss: 55890.59765625]\n",
      "[Epoch 9/100] [Batch 200/782] [D loss: -115376.59375] [G loss: 58337.6640625]\n",
      "[Epoch 9/100] [Batch 300/782] [D loss: -121261.5859375] [G loss: 61037.66015625]\n",
      "[Epoch 9/100] [Batch 400/782] [D loss: -126049.3671875] [G loss: 63569.36328125]\n",
      "[Epoch 9/100] [Batch 500/782] [D loss: -129611.375] [G loss: 65415.36328125]\n",
      "[Epoch 9/100] [Batch 600/782] [D loss: -134972.625] [G loss: 67886.671875]\n",
      "[Epoch 9/100] [Batch 700/782] [D loss: -140296.515625] [G loss: 70541.53125]\n",
      "[Epoch 10/100] [Batch 0/782] [D loss: -141038.453125] [G loss: 71500.140625]\n",
      "[Epoch 10/100] [Batch 100/782] [D loss: -147746.078125] [G loss: 74314.375]\n",
      "[Epoch 10/100] [Batch 200/782] [D loss: -152595.671875] [G loss: 76801.078125]\n",
      "[Epoch 10/100] [Batch 300/782] [D loss: -157225.390625] [G loss: 79279.046875]\n",
      "[Epoch 10/100] [Batch 400/782] [D loss: -162022.765625] [G loss: 81606.6328125]\n",
      "[Epoch 10/100] [Batch 500/782] [D loss: -167771.890625] [G loss: 84212.921875]\n",
      "[Epoch 10/100] [Batch 600/782] [D loss: -172959.25] [G loss: 86964.609375]\n",
      "[Epoch 10/100] [Batch 700/782] [D loss: -177872.859375] [G loss: 89322.21875]\n",
      "[Epoch 11/100] [Batch 0/782] [D loss: 216.46142578125] [G loss: -671.6821899414062]\n",
      "[Epoch 11/100] [Batch 100/782] [D loss: -185108.359375] [G loss: 93159.0546875]\n",
      "[Epoch 11/100] [Batch 200/782] [D loss: -191973.1875] [G loss: 96271.296875]\n",
      "[Epoch 11/100] [Batch 300/782] [D loss: -197019.546875] [G loss: 98779.125]\n",
      "[Epoch 11/100] [Batch 400/782] [D loss: -88103.125] [G loss: -65916.46875]\n",
      "[Epoch 11/100] [Batch 500/782] [D loss: 1443.192138671875] [G loss: -831.435302734375]\n",
      "[Epoch 11/100] [Batch 600/782] [D loss: -208053.875] [G loss: 104469.6640625]\n",
      "[Epoch 11/100] [Batch 700/782] [D loss: -214330.28125] [G loss: 107978.59375]\n",
      "[Epoch 12/100] [Batch 0/782] [D loss: -107155.734375] [G loss: 76799.0703125]\n",
      "[Epoch 12/100] [Batch 100/782] [D loss: -226859.875] [G loss: 113750.3515625]\n",
      "[Epoch 12/100] [Batch 200/782] [D loss: -228893.9375] [G loss: 115280.90625]\n",
      "[Epoch 12/100] [Batch 300/782] [D loss: -233220.046875] [G loss: 117587.9375]\n",
      "[Epoch 12/100] [Batch 400/782] [D loss: -241936.578125] [G loss: 121666.2890625]\n",
      "[Epoch 12/100] [Batch 500/782] [D loss: -247870.0] [G loss: 125023.40625]\n",
      "[Epoch 12/100] [Batch 600/782] [D loss: -255133.609375] [G loss: 128204.546875]\n",
      "[Epoch 12/100] [Batch 700/782] [D loss: -255927.296875] [G loss: 129471.34375]\n",
      "[Epoch 13/100] [Batch 0/782] [D loss: 111078.8046875] [G loss: -2772.852294921875]\n",
      "[Epoch 13/100] [Batch 100/782] [D loss: 750.7868041992188] [G loss: -399.24517822265625]\n",
      "[Epoch 13/100] [Batch 200/782] [D loss: 1290.61962890625] [G loss: -604.7719116210938]\n",
      "[Epoch 13/100] [Batch 300/782] [D loss: 759.025634765625] [G loss: -913.0377197265625]\n",
      "[Epoch 13/100] [Batch 400/782] [D loss: 277.76678466796875] [G loss: -1069.9951171875]\n",
      "[Epoch 13/100] [Batch 500/782] [D loss: 1423.3863525390625] [G loss: -877.8657836914062]\n",
      "[Epoch 13/100] [Batch 600/782] [D loss: 598.573974609375] [G loss: -675.4461669921875]\n",
      "[Epoch 13/100] [Batch 700/782] [D loss: 483.77032470703125] [G loss: -751.887939453125]\n",
      "[Epoch 14/100] [Batch 0/782] [D loss: 443.9715576171875] [G loss: -679.5836181640625]\n",
      "[Epoch 14/100] [Batch 100/782] [D loss: 148.04666137695312] [G loss: -714.354248046875]\n",
      "[Epoch 14/100] [Batch 200/782] [D loss: 143.54840087890625] [G loss: -631.0555419921875]\n",
      "[Epoch 14/100] [Batch 300/782] [D loss: 262.037841796875] [G loss: -601.4689331054688]\n",
      "[Epoch 14/100] [Batch 400/782] [D loss: 147.42701721191406] [G loss: -656.095458984375]\n",
      "[Epoch 14/100] [Batch 500/782] [D loss: 44.781097412109375] [G loss: -573.4078369140625]\n",
      "[Epoch 14/100] [Batch 600/782] [D loss: -90.55812072753906] [G loss: -330.5487365722656]\n",
      "[Epoch 14/100] [Batch 700/782] [D loss: 492.14752197265625] [G loss: -563.2897338867188]\n",
      "[Epoch 15/100] [Batch 0/782] [D loss: 679.4041748046875] [G loss: -709.5701293945312]\n",
      "[Epoch 15/100] [Batch 100/782] [D loss: 1118.510009765625] [G loss: -2524.59521484375]\n",
      "[Epoch 15/100] [Batch 200/782] [D loss: 372.69488525390625] [G loss: -275.8443603515625]\n",
      "[Epoch 15/100] [Batch 300/782] [D loss: 500.73358154296875] [G loss: -835.664794921875]\n",
      "[Epoch 15/100] [Batch 400/782] [D loss: 269.28271484375] [G loss: -396.6706237792969]\n",
      "[Epoch 15/100] [Batch 500/782] [D loss: 139.42257690429688] [G loss: -622.19140625]\n",
      "[Epoch 15/100] [Batch 600/782] [D loss: 6.9921112060546875] [G loss: -648.533203125]\n",
      "[Epoch 15/100] [Batch 700/782] [D loss: 200.0553741455078] [G loss: -730.6718139648438]\n",
      "[Epoch 16/100] [Batch 0/782] [D loss: 437.36517333984375] [G loss: -633.8895263671875]\n",
      "[Epoch 16/100] [Batch 100/782] [D loss: 41.17671203613281] [G loss: -546.78955078125]\n",
      "[Epoch 16/100] [Batch 200/782] [D loss: 62.855682373046875] [G loss: -603.016357421875]\n",
      "[Epoch 16/100] [Batch 300/782] [D loss: 52.503448486328125] [G loss: -573.2562866210938]\n",
      "[Epoch 16/100] [Batch 400/782] [D loss: 190.14971923828125] [G loss: -528.3101196289062]\n",
      "[Epoch 16/100] [Batch 500/782] [D loss: 687.825927734375] [G loss: -610.171630859375]\n",
      "[Epoch 16/100] [Batch 600/782] [D loss: 100.3773193359375] [G loss: -552.390869140625]\n",
      "[Epoch 16/100] [Batch 700/782] [D loss: 355.37261962890625] [G loss: -353.5167541503906]\n",
      "[Epoch 17/100] [Batch 0/782] [D loss: 77.77008056640625] [G loss: 63.277835845947266]\n",
      "[Epoch 17/100] [Batch 100/782] [D loss: 39.390716552734375] [G loss: -246.51007080078125]\n",
      "[Epoch 17/100] [Batch 200/782] [D loss: 159.9379425048828] [G loss: -550.0813598632812]\n",
      "[Epoch 17/100] [Batch 300/782] [D loss: 84.80011749267578] [G loss: -565.9711303710938]\n",
      "[Epoch 17/100] [Batch 400/782] [D loss: 286.000732421875] [G loss: -562.834716796875]\n",
      "[Epoch 17/100] [Batch 500/782] [D loss: 284.49560546875] [G loss: -507.17510986328125]\n",
      "[Epoch 17/100] [Batch 600/782] [D loss: 43.08393859863281] [G loss: -487.35784912109375]\n",
      "[Epoch 17/100] [Batch 700/782] [D loss: 38.78718566894531] [G loss: -469.00897216796875]\n",
      "[Epoch 18/100] [Batch 0/782] [D loss: -170.2296142578125] [G loss: -204.8118896484375]\n",
      "[Epoch 18/100] [Batch 100/782] [D loss: -37.299560546875] [G loss: 1457.0963134765625]\n",
      "[Epoch 18/100] [Batch 200/782] [D loss: 301.805908203125] [G loss: -639.1434936523438]\n",
      "[Epoch 18/100] [Batch 300/782] [D loss: 121.90097045898438] [G loss: -517.539794921875]\n",
      "[Epoch 18/100] [Batch 400/782] [D loss: 246.5369873046875] [G loss: -561.1209716796875]\n",
      "[Epoch 18/100] [Batch 500/782] [D loss: 161.055908203125] [G loss: -566.449462890625]\n",
      "[Epoch 18/100] [Batch 600/782] [D loss: 118.09703063964844] [G loss: -591.49462890625]\n",
      "[Epoch 18/100] [Batch 700/782] [D loss: 283.5618591308594] [G loss: -637.4266967773438]\n",
      "[Epoch 19/100] [Batch 0/782] [D loss: 488.2763671875] [G loss: -778.1130981445312]\n",
      "[Epoch 19/100] [Batch 100/782] [D loss: 233.95556640625] [G loss: -590.6928100585938]\n",
      "[Epoch 19/100] [Batch 200/782] [D loss: 243.2479248046875] [G loss: -635.3614501953125]\n",
      "[Epoch 19/100] [Batch 300/782] [D loss: 207.7091827392578] [G loss: -628.7669677734375]\n",
      "[Epoch 19/100] [Batch 400/782] [D loss: 188.3414764404297] [G loss: -592.0802001953125]\n",
      "[Epoch 19/100] [Batch 500/782] [D loss: 180.68710327148438] [G loss: -586.1635131835938]\n",
      "[Epoch 19/100] [Batch 600/782] [D loss: 322.49493408203125] [G loss: -673.3630981445312]\n",
      "[Epoch 19/100] [Batch 700/782] [D loss: 328.4675598144531] [G loss: -465.2578125]\n",
      "[Epoch 20/100] [Batch 0/782] [D loss: 319.62060546875] [G loss: -558.7733764648438]\n",
      "[Epoch 20/100] [Batch 100/782] [D loss: 316.7509765625] [G loss: -691.867919921875]\n",
      "[Epoch 20/100] [Batch 200/782] [D loss: 164.3271942138672] [G loss: -641.2014770507812]\n",
      "[Epoch 20/100] [Batch 300/782] [D loss: 150.6295166015625] [G loss: -580.0783081054688]\n",
      "[Epoch 20/100] [Batch 400/782] [D loss: 443.10357666015625] [G loss: -563.6787109375]\n",
      "[Epoch 20/100] [Batch 500/782] [D loss: 178.94131469726562] [G loss: -672.5897216796875]\n",
      "[Epoch 20/100] [Batch 600/782] [D loss: 368.992431640625] [G loss: -831.7139282226562]\n",
      "[Epoch 20/100] [Batch 700/782] [D loss: 124.08998107910156] [G loss: -613.635986328125]\n",
      "[Epoch 21/100] [Batch 0/782] [D loss: 545.4890747070312] [G loss: -942.5411376953125]\n",
      "[Epoch 21/100] [Batch 100/782] [D loss: 155.30926513671875] [G loss: -791.690185546875]\n",
      "[Epoch 21/100] [Batch 200/782] [D loss: 97.59043884277344] [G loss: -1139.313720703125]\n",
      "[Epoch 21/100] [Batch 300/782] [D loss: 96.19862365722656] [G loss: -1281.1104736328125]\n",
      "[Epoch 21/100] [Batch 400/782] [D loss: 21.71435546875] [G loss: -1222.5711669921875]\n",
      "[Epoch 21/100] [Batch 500/782] [D loss: -34.92710876464844] [G loss: -1189.542724609375]\n",
      "[Epoch 21/100] [Batch 600/782] [D loss: -140.34732055664062] [G loss: -1392.7069091796875]\n",
      "[Epoch 21/100] [Batch 700/782] [D loss: 84.77035522460938] [G loss: -990.0579833984375]\n",
      "[Epoch 22/100] [Batch 0/782] [D loss: 53.08091735839844] [G loss: -995.2998046875]\n",
      "[Epoch 22/100] [Batch 100/782] [D loss: -73.08467864990234] [G loss: -1203.192626953125]\n",
      "[Epoch 22/100] [Batch 200/782] [D loss: 73.54539489746094] [G loss: -1151.09033203125]\n",
      "[Epoch 22/100] [Batch 300/782] [D loss: 19.30908203125] [G loss: -679.891357421875]\n",
      "[Epoch 22/100] [Batch 400/782] [D loss: -178.32659912109375] [G loss: -703.825927734375]\n",
      "[Epoch 22/100] [Batch 500/782] [D loss: -62.00556945800781] [G loss: -942.256103515625]\n",
      "[Epoch 22/100] [Batch 600/782] [D loss: -207.6175079345703] [G loss: -865.412109375]\n",
      "[Epoch 22/100] [Batch 700/782] [D loss: 115.16614532470703] [G loss: -871.6025390625]\n",
      "[Epoch 23/100] [Batch 0/782] [D loss: -55.14369201660156] [G loss: -271.1209716796875]\n",
      "[Epoch 23/100] [Batch 100/782] [D loss: -214.75901794433594] [G loss: -614.0113525390625]\n",
      "[Epoch 23/100] [Batch 200/782] [D loss: 33.61811828613281] [G loss: -1095.9434814453125]\n",
      "[Epoch 23/100] [Batch 300/782] [D loss: 63.072242736816406] [G loss: -879.1109619140625]\n",
      "[Epoch 23/100] [Batch 400/782] [D loss: -701.8162231445312] [G loss: -664.6876831054688]\n",
      "[Epoch 23/100] [Batch 500/782] [D loss: -399.308349609375] [G loss: -988.6199951171875]\n",
      "[Epoch 23/100] [Batch 600/782] [D loss: -985.3779296875] [G loss: -555.9276123046875]\n",
      "[Epoch 23/100] [Batch 700/782] [D loss: -622.29052734375] [G loss: -2681.251953125]\n",
      "[Epoch 24/100] [Batch 0/782] [D loss: 1384.5040283203125] [G loss: 1887.96630859375]\n",
      "[Epoch 24/100] [Batch 100/782] [D loss: -59.3385009765625] [G loss: -606.4468383789062]\n",
      "[Epoch 24/100] [Batch 200/782] [D loss: -820.4368286132812] [G loss: -309.8697814941406]\n",
      "[Epoch 24/100] [Batch 300/782] [D loss: 25.34345245361328] [G loss: -964.832275390625]\n",
      "[Epoch 24/100] [Batch 400/782] [D loss: -230.0487060546875] [G loss: -569.8460693359375]\n",
      "[Epoch 24/100] [Batch 500/782] [D loss: 161.84927368164062] [G loss: -785.8594970703125]\n",
      "[Epoch 24/100] [Batch 600/782] [D loss: -10.942070007324219] [G loss: -625.3892822265625]\n",
      "[Epoch 24/100] [Batch 700/782] [D loss: -551.2590942382812] [G loss: -850.92578125]\n",
      "[Epoch 25/100] [Batch 0/782] [D loss: 10.124107360839844] [G loss: -522.0523681640625]\n",
      "[Epoch 25/100] [Batch 100/782] [D loss: -781.7154541015625] [G loss: 84.64942932128906]\n",
      "[Epoch 25/100] [Batch 200/782] [D loss: 60.9747314453125] [G loss: -552.7658081054688]\n",
      "[Epoch 25/100] [Batch 300/782] [D loss: -3509.765869140625] [G loss: 106.20975494384766]\n",
      "[Epoch 25/100] [Batch 400/782] [D loss: -3899.195556640625] [G loss: 2219.6494140625]\n",
      "[Epoch 25/100] [Batch 500/782] [D loss: 180.45291137695312] [G loss: -1648.463623046875]\n",
      "[Epoch 25/100] [Batch 600/782] [D loss: -67.17559814453125] [G loss: -4407.064453125]\n",
      "[Epoch 25/100] [Batch 700/782] [D loss: -1941.644775390625] [G loss: 1258.1187744140625]\n",
      "[Epoch 26/100] [Batch 0/782] [D loss: 62.464630126953125] [G loss: -502.42352294921875]\n",
      "[Epoch 26/100] [Batch 100/782] [D loss: -6179.748046875] [G loss: 10610.931640625]\n",
      "[Epoch 26/100] [Batch 200/782] [D loss: -18891.759765625] [G loss: -5413.5869140625]\n",
      "[Epoch 26/100] [Batch 300/782] [D loss: -26349.755859375] [G loss: 7977.25927734375]\n",
      "[Epoch 26/100] [Batch 400/782] [D loss: 657.9476928710938] [G loss: 1124.9344482421875]\n",
      "[Epoch 26/100] [Batch 500/782] [D loss: -9814.015625] [G loss: 29132.6484375]\n",
      "[Epoch 26/100] [Batch 600/782] [D loss: -9018.7529296875] [G loss: 28265.5859375]\n",
      "[Epoch 26/100] [Batch 700/782] [D loss: -2669.71630859375] [G loss: 270.03173828125]\n",
      "[Epoch 27/100] [Batch 0/782] [D loss: -22160.75] [G loss: 40403.40625]\n",
      "[Epoch 27/100] [Batch 100/782] [D loss: -1731.470703125] [G loss: 4485.09814453125]\n",
      "[Epoch 27/100] [Batch 200/782] [D loss: -59407.08203125] [G loss: 65174.2734375]\n",
      "[Epoch 27/100] [Batch 300/782] [D loss: -82785.8203125] [G loss: 66800.953125]\n",
      "[Epoch 27/100] [Batch 400/782] [D loss: 3834.54931640625] [G loss: -1724.4053955078125]\n",
      "[Epoch 27/100] [Batch 500/782] [D loss: -15684.044921875] [G loss: 290.3359375]\n",
      "[Epoch 27/100] [Batch 600/782] [D loss: -77804.8046875] [G loss: 42233.7421875]\n",
      "[Epoch 27/100] [Batch 700/782] [D loss: -12187.5517578125] [G loss: -10285.84765625]\n",
      "[Epoch 28/100] [Batch 0/782] [D loss: 4937.8154296875] [G loss: -14191.0673828125]\n",
      "[Epoch 28/100] [Batch 100/782] [D loss: 4079.693359375] [G loss: 81281.8359375]\n",
      "[Epoch 28/100] [Batch 200/782] [D loss: -20276.45703125] [G loss: -22286.65234375]\n",
      "[Epoch 28/100] [Batch 300/782] [D loss: -46726.9765625] [G loss: 58596.765625]\n",
      "[Epoch 28/100] [Batch 400/782] [D loss: 3333.87109375] [G loss: 29608.84375]\n",
      "[Epoch 28/100] [Batch 500/782] [D loss: -35067.15234375] [G loss: 41006.765625]\n",
      "[Epoch 28/100] [Batch 600/782] [D loss: -76567.40625] [G loss: 93529.65625]\n",
      "[Epoch 28/100] [Batch 700/782] [D loss: -201047.046875] [G loss: 99068.859375]\n",
      "[Epoch 29/100] [Batch 0/782] [D loss: -6031.01123046875] [G loss: 29190.708984375]\n",
      "[Epoch 29/100] [Batch 100/782] [D loss: 2261.10498046875] [G loss: 13888.568359375]\n",
      "[Epoch 29/100] [Batch 200/782] [D loss: -102446.453125] [G loss: 75085.96875]\n",
      "[Epoch 29/100] [Batch 300/782] [D loss: -123077.8515625] [G loss: 48567.6640625]\n",
      "[Epoch 29/100] [Batch 400/782] [D loss: -39517.32421875] [G loss: 17031.93359375]\n",
      "[Epoch 29/100] [Batch 500/782] [D loss: -35464.41015625] [G loss: 82616.984375]\n",
      "[Epoch 29/100] [Batch 600/782] [D loss: -209319.40625] [G loss: 113488.8125]\n",
      "[Epoch 29/100] [Batch 700/782] [D loss: -87821.375] [G loss: 84583.8203125]\n",
      "[Epoch 30/100] [Batch 0/782] [D loss: 82800.671875] [G loss: -33356.4375]\n",
      "[Epoch 30/100] [Batch 100/782] [D loss: -12743.359375] [G loss: -95408.046875]\n",
      "[Epoch 30/100] [Batch 200/782] [D loss: -217862.3125] [G loss: 119840.6171875]\n",
      "[Epoch 30/100] [Batch 300/782] [D loss: -224474.8125] [G loss: 119102.625]\n",
      "[Epoch 30/100] [Batch 400/782] [D loss: -213155.578125] [G loss: 117159.703125]\n",
      "[Epoch 30/100] [Batch 500/782] [D loss: -126147.0] [G loss: 125876.671875]\n",
      "[Epoch 30/100] [Batch 600/782] [D loss: -229215.078125] [G loss: 124655.1796875]\n",
      "[Epoch 30/100] [Batch 700/782] [D loss: -98312.2578125] [G loss: -78663.921875]\n",
      "[Epoch 31/100] [Batch 0/782] [D loss: 80577.1875] [G loss: 12199.9501953125]\n",
      "[Epoch 31/100] [Batch 100/782] [D loss: 4090.086669921875] [G loss: 1269.0810546875]\n",
      "[Epoch 31/100] [Batch 200/782] [D loss: 2272.916015625] [G loss: 238.68418884277344]\n",
      "[Epoch 31/100] [Batch 300/782] [D loss: 1334.465087890625] [G loss: -287.24688720703125]\n",
      "[Epoch 31/100] [Batch 400/782] [D loss: 1044.7470703125] [G loss: -601.97021484375]\n",
      "[Epoch 31/100] [Batch 500/782] [D loss: 553.3174438476562] [G loss: -906.7113647460938]\n",
      "[Epoch 31/100] [Batch 600/782] [D loss: 400.21795654296875] [G loss: -1131.454833984375]\n",
      "[Epoch 31/100] [Batch 700/782] [D loss: 625.825439453125] [G loss: -1103.620849609375]\n",
      "[Epoch 32/100] [Batch 0/782] [D loss: 184.09625244140625] [G loss: -1071.96630859375]\n",
      "[Epoch 32/100] [Batch 100/782] [D loss: -92.7677001953125] [G loss: -1287.4735107421875]\n",
      "[Epoch 32/100] [Batch 200/782] [D loss: 297.0303955078125] [G loss: -1097.5523681640625]\n",
      "[Epoch 32/100] [Batch 300/782] [D loss: -1267.14208984375] [G loss: -1157.258056640625]\n",
      "[Epoch 32/100] [Batch 400/782] [D loss: -2080.100341796875] [G loss: -825.060302734375]\n",
      "[Epoch 32/100] [Batch 500/782] [D loss: -1388.0062255859375] [G loss: -359.2413330078125]\n",
      "[Epoch 32/100] [Batch 600/782] [D loss: -2627.398681640625] [G loss: 1873.1884765625]\n",
      "[Epoch 32/100] [Batch 700/782] [D loss: 3321.07421875] [G loss: -9686.875]\n",
      "[Epoch 33/100] [Batch 0/782] [D loss: -19633.625] [G loss: 47108.15625]\n",
      "[Epoch 33/100] [Batch 100/782] [D loss: -68003.3984375] [G loss: 652.2532348632812]\n",
      "[Epoch 33/100] [Batch 200/782] [D loss: -14874.3779296875] [G loss: 52149.81640625]\n",
      "[Epoch 33/100] [Batch 300/782] [D loss: -74264.859375] [G loss: 54263.125]\n",
      "[Epoch 33/100] [Batch 400/782] [D loss: -106517.6328125] [G loss: 100166.203125]\n",
      "[Epoch 33/100] [Batch 500/782] [D loss: -65021.23046875] [G loss: 88044.390625]\n",
      "[Epoch 33/100] [Batch 600/782] [D loss: -214179.890625] [G loss: 138126.046875]\n",
      "[Epoch 33/100] [Batch 700/782] [D loss: -260587.0625] [G loss: 150442.15625]\n",
      "[Epoch 34/100] [Batch 0/782] [D loss: -283452.28125] [G loss: 147999.109375]\n",
      "[Epoch 34/100] [Batch 100/782] [D loss: -115120.296875] [G loss: 53023.1015625]\n",
      "[Epoch 34/100] [Batch 200/782] [D loss: 5572.341796875] [G loss: -128051.3125]\n",
      "[Epoch 34/100] [Batch 300/782] [D loss: -301970.875] [G loss: 157240.84375]\n",
      "[Epoch 34/100] [Batch 400/782] [D loss: -295271.21875] [G loss: 151328.125]\n",
      "[Epoch 34/100] [Batch 500/782] [D loss: -309668.375] [G loss: 160805.28125]\n",
      "[Epoch 34/100] [Batch 600/782] [D loss: -46358.90625] [G loss: -67336.21875]\n",
      "[Epoch 34/100] [Batch 700/782] [D loss: -226433.96875] [G loss: 164620.1875]\n",
      "[Epoch 35/100] [Batch 0/782] [D loss: 3943.728515625] [G loss: -250.55120849609375]\n",
      "[Epoch 35/100] [Batch 100/782] [D loss: -17540.146484375] [G loss: 4904.216796875]\n",
      "[Epoch 35/100] [Batch 200/782] [D loss: -115173.84375] [G loss: 77453.8828125]\n",
      "[Epoch 35/100] [Batch 300/782] [D loss: -233628.78125] [G loss: 159362.34375]\n",
      "[Epoch 35/100] [Batch 400/782] [D loss: -268521.25] [G loss: 153777.625]\n",
      "[Epoch 35/100] [Batch 500/782] [D loss: -131563.546875] [G loss: 118975.703125]\n",
      "[Epoch 35/100] [Batch 600/782] [D loss: -81535.203125] [G loss: 138478.96875]\n",
      "[Epoch 35/100] [Batch 700/782] [D loss: -229560.234375] [G loss: 108300.0546875]\n",
      "[Epoch 36/100] [Batch 0/782] [D loss: 4027.77734375] [G loss: 129249.5234375]\n",
      "[Epoch 36/100] [Batch 100/782] [D loss: -36867.2265625] [G loss: 31432.4453125]\n",
      "[Epoch 36/100] [Batch 200/782] [D loss: 4595.76953125] [G loss: -1844.8353271484375]\n",
      "[Epoch 36/100] [Batch 300/782] [D loss: 1998.29296875] [G loss: -2823.0546875]\n",
      "[Epoch 36/100] [Batch 400/782] [D loss: 1315.6319580078125] [G loss: -3386.35595703125]\n",
      "[Epoch 36/100] [Batch 500/782] [D loss: 1593.126220703125] [G loss: -3957.290283203125]\n",
      "[Epoch 36/100] [Batch 600/782] [D loss: 810.1636352539062] [G loss: -4587.72314453125]\n",
      "[Epoch 36/100] [Batch 700/782] [D loss: 659.1251831054688] [G loss: -4925.8828125]\n",
      "[Epoch 37/100] [Batch 0/782] [D loss: 952.363525390625] [G loss: -5281.9462890625]\n",
      "[Epoch 37/100] [Batch 100/782] [D loss: 330.08245849609375] [G loss: -5672.8037109375]\n",
      "[Epoch 37/100] [Batch 200/782] [D loss: 299.32427978515625] [G loss: -5933.6396484375]\n",
      "[Epoch 37/100] [Batch 300/782] [D loss: 581.8278198242188] [G loss: -6258.12060546875]\n",
      "[Epoch 37/100] [Batch 400/782] [D loss: 68.88027954101562] [G loss: -6405.5263671875]\n",
      "[Epoch 37/100] [Batch 500/782] [D loss: 577.8955078125] [G loss: -6767.431640625]\n",
      "[Epoch 37/100] [Batch 600/782] [D loss: -552.780517578125] [G loss: -4547.6796875]\n",
      "[Epoch 37/100] [Batch 700/782] [D loss: -255.2965087890625] [G loss: -4450.6259765625]\n",
      "[Epoch 38/100] [Batch 0/782] [D loss: -8086.36474609375] [G loss: -12651.6494140625]\n",
      "[Epoch 38/100] [Batch 100/782] [D loss: 4704.18212890625] [G loss: -4280.1669921875]\n",
      "[Epoch 38/100] [Batch 200/782] [D loss: 22637.03515625] [G loss: 129670.015625]\n",
      "[Epoch 38/100] [Batch 300/782] [D loss: -319990.6875] [G loss: 177764.96875]\n",
      "[Epoch 38/100] [Batch 400/782] [D loss: -5961.9033203125] [G loss: 5773.9052734375]\n",
      "[Epoch 38/100] [Batch 500/782] [D loss: -148870.703125] [G loss: -141981.5625]\n",
      "[Epoch 38/100] [Batch 600/782] [D loss: -130509.0234375] [G loss: 157205.71875]\n",
      "[Epoch 38/100] [Batch 700/782] [D loss: -374595.5] [G loss: 193712.375]\n",
      "[Epoch 39/100] [Batch 0/782] [D loss: -5466.408203125] [G loss: 185354.265625]\n",
      "[Epoch 39/100] [Batch 100/782] [D loss: -316277.75] [G loss: 188667.578125]\n",
      "[Epoch 39/100] [Batch 200/782] [D loss: -180706.40625] [G loss: 184009.40625]\n",
      "[Epoch 39/100] [Batch 300/782] [D loss: -100673.125] [G loss: 27936.248046875]\n",
      "[Epoch 39/100] [Batch 400/782] [D loss: -29828.345703125] [G loss: 80160.953125]\n",
      "[Epoch 39/100] [Batch 500/782] [D loss: -68212.359375] [G loss: 98138.078125]\n",
      "[Epoch 39/100] [Batch 600/782] [D loss: -402809.59375] [G loss: 205671.03125]\n",
      "[Epoch 39/100] [Batch 700/782] [D loss: -195.5423583984375] [G loss: -24223.099609375]\n",
      "[Epoch 40/100] [Batch 0/782] [D loss: -235317.5625] [G loss: 210989.84375]\n",
      "[Epoch 40/100] [Batch 100/782] [D loss: 4691.828125] [G loss: -146699.9375]\n",
      "[Epoch 40/100] [Batch 200/782] [D loss: -78533.7109375] [G loss: -51600.45703125]\n",
      "[Epoch 40/100] [Batch 300/782] [D loss: -62389.3203125] [G loss: 36530.07421875]\n",
      "[Epoch 40/100] [Batch 400/782] [D loss: -339309.65625] [G loss: 213166.6875]\n",
      "[Epoch 40/100] [Batch 500/782] [D loss: -131349.796875] [G loss: 128333.9140625]\n",
      "[Epoch 40/100] [Batch 600/782] [D loss: -277270.03125] [G loss: 185557.71875]\n",
      "[Epoch 40/100] [Batch 700/782] [D loss: -295331.78125] [G loss: 161068.84375]\n",
      "[Epoch 41/100] [Batch 0/782] [D loss: -7186.5703125] [G loss: -163103.421875]\n",
      "[Epoch 41/100] [Batch 100/782] [D loss: -68619.8046875] [G loss: 208751.578125]\n",
      "[Epoch 41/100] [Batch 200/782] [D loss: -392720.625] [G loss: 195943.90625]\n",
      "[Epoch 41/100] [Batch 300/782] [D loss: -399242.40625] [G loss: 218028.53125]\n",
      "[Epoch 41/100] [Batch 400/782] [D loss: -7531.99072265625] [G loss: 203426.0625]\n",
      "[Epoch 41/100] [Batch 500/782] [D loss: 14008.30859375] [G loss: -165082.71875]\n",
      "[Epoch 41/100] [Batch 600/782] [D loss: -285106.6875] [G loss: 203665.46875]\n",
      "[Epoch 41/100] [Batch 700/782] [D loss: 8470.69921875] [G loss: -227421.453125]\n",
      "[Epoch 42/100] [Batch 0/782] [D loss: 323.460693359375] [G loss: -167613.03125]\n",
      "[Epoch 42/100] [Batch 100/782] [D loss: -310827.78125] [G loss: 180949.1875]\n",
      "[Epoch 42/100] [Batch 200/782] [D loss: 64.572509765625] [G loss: -214045.671875]\n",
      "[Epoch 42/100] [Batch 300/782] [D loss: -77307.171875] [G loss: 157331.96875]\n",
      "[Epoch 42/100] [Batch 400/782] [D loss: -62845.57421875] [G loss: 203085.4375]\n",
      "[Epoch 42/100] [Batch 500/782] [D loss: -485971.84375] [G loss: 247807.015625]\n",
      "[Epoch 42/100] [Batch 600/782] [D loss: -6354.84375] [G loss: -191536.4375]\n",
      "[Epoch 42/100] [Batch 700/782] [D loss: -113304.8359375] [G loss: 169159.703125]\n",
      "[Epoch 43/100] [Batch 0/782] [D loss: -179341.46875] [G loss: 107859.3125]\n",
      "[Epoch 43/100] [Batch 100/782] [D loss: -384198.5625] [G loss: 237737.34375]\n",
      "[Epoch 43/100] [Batch 200/782] [D loss: -528213.625] [G loss: 267411.5625]\n",
      "[Epoch 43/100] [Batch 300/782] [D loss: -12330.880859375] [G loss: -194762.90625]\n",
      "[Epoch 43/100] [Batch 400/782] [D loss: 903.65625] [G loss: -155174.0625]\n",
      "[Epoch 43/100] [Batch 500/782] [D loss: -81321.8671875] [G loss: 186972.03125]\n",
      "[Epoch 43/100] [Batch 600/782] [D loss: -274288.1875] [G loss: 75318.7890625]\n",
      "[Epoch 43/100] [Batch 700/782] [D loss: -242185.71875] [G loss: 256683.25]\n",
      "[Epoch 44/100] [Batch 0/782] [D loss: 3257.76806640625] [G loss: 262756.3125]\n",
      "[Epoch 44/100] [Batch 100/782] [D loss: -423247.9375] [G loss: 201635.328125]\n",
      "[Epoch 44/100] [Batch 200/782] [D loss: -570423.125] [G loss: 288190.1875]\n",
      "[Epoch 44/100] [Batch 300/782] [D loss: -496708.75] [G loss: 260056.890625]\n",
      "[Epoch 44/100] [Batch 400/782] [D loss: -270698.03125] [G loss: 240770.375]\n",
      "[Epoch 44/100] [Batch 500/782] [D loss: 181516.09375] [G loss: 281968.9375]\n",
      "[Epoch 44/100] [Batch 600/782] [D loss: 22321.419921875] [G loss: 17201.51953125]\n",
      "[Epoch 44/100] [Batch 700/782] [D loss: -351657.15625] [G loss: 197758.15625]\n",
      "[Epoch 45/100] [Batch 0/782] [D loss: -581563.875] [G loss: 298834.21875]\n",
      "[Epoch 45/100] [Batch 100/782] [D loss: -7142.68408203125] [G loss: -274407.875]\n",
      "[Epoch 45/100] [Batch 200/782] [D loss: -192220.203125] [G loss: 184318.84375]\n",
      "[Epoch 45/100] [Batch 300/782] [D loss: -28242.98046875] [G loss: 201861.625]\n",
      "[Epoch 45/100] [Batch 400/782] [D loss: -504643.96875] [G loss: 293723.5]\n",
      "[Epoch 45/100] [Batch 500/782] [D loss: -37469.17578125] [G loss: 270839.0625]\n",
      "[Epoch 45/100] [Batch 600/782] [D loss: -88777.1640625] [G loss: 52112.921875]\n",
      "[Epoch 45/100] [Batch 700/782] [D loss: 4672.115234375] [G loss: -291907.65625]\n",
      "[Epoch 46/100] [Batch 0/782] [D loss: -34590.1484375] [G loss: 44698.88671875]\n",
      "[Epoch 46/100] [Batch 100/782] [D loss: -166347.9375] [G loss: 238304.84375]\n",
      "[Epoch 46/100] [Batch 200/782] [D loss: -559363.875] [G loss: 298522.625]\n",
      "[Epoch 46/100] [Batch 300/782] [D loss: -572692.125] [G loss: 302272.78125]\n",
      "[Epoch 46/100] [Batch 400/782] [D loss: -5513.05859375] [G loss: -265322.78125]\n",
      "[Epoch 46/100] [Batch 500/782] [D loss: -10342.3447265625] [G loss: -272288.375]\n",
      "[Epoch 46/100] [Batch 600/782] [D loss: -631.334716796875] [G loss: -271082.4375]\n",
      "[Epoch 46/100] [Batch 700/782] [D loss: -6974.9443359375] [G loss: -51902.8828125]\n",
      "[Epoch 47/100] [Batch 0/782] [D loss: 6833.94189453125] [G loss: 304823.8125]\n",
      "[Epoch 47/100] [Batch 100/782] [D loss: -44629.34375] [G loss: 138528.625]\n",
      "[Epoch 47/100] [Batch 200/782] [D loss: -394654.75] [G loss: 260847.90625]\n",
      "[Epoch 47/100] [Batch 300/782] [D loss: -126951.46875] [G loss: 213997.015625]\n",
      "[Epoch 47/100] [Batch 400/782] [D loss: -221074.671875] [G loss: 208989.390625]\n",
      "[Epoch 47/100] [Batch 500/782] [D loss: -595181.0625] [G loss: 303722.6875]\n",
      "[Epoch 47/100] [Batch 600/782] [D loss: -4901.5537109375] [G loss: -218615.875]\n",
      "[Epoch 47/100] [Batch 700/782] [D loss: 299.77734375] [G loss: -42127.984375]\n",
      "[Epoch 48/100] [Batch 0/782] [D loss: -175597.265625] [G loss: 264886.625]\n",
      "[Epoch 48/100] [Batch 100/782] [D loss: -687673.3125] [G loss: 348959.9375]\n",
      "[Epoch 48/100] [Batch 200/782] [D loss: -9597.6279296875] [G loss: -259729.625]\n",
      "[Epoch 48/100] [Batch 300/782] [D loss: -46514.00390625] [G loss: -182936.484375]\n",
      "[Epoch 48/100] [Batch 400/782] [D loss: -601456.625] [G loss: 322555.125]\n",
      "[Epoch 48/100] [Batch 500/782] [D loss: -213195.65625] [G loss: 281683.3125]\n",
      "[Epoch 48/100] [Batch 600/782] [D loss: -657680.9375] [G loss: 351137.6875]\n",
      "[Epoch 48/100] [Batch 700/782] [D loss: -704991.1875] [G loss: 358919.09375]\n",
      "[Epoch 49/100] [Batch 0/782] [D loss: 182572.03125] [G loss: -35405.765625]\n",
      "[Epoch 49/100] [Batch 100/782] [D loss: 613.6376953125] [G loss: -36022.51953125]\n",
      "[Epoch 49/100] [Batch 200/782] [D loss: 694.219970703125] [G loss: -35335.00390625]\n",
      "[Epoch 49/100] [Batch 300/782] [D loss: -683.336669921875] [G loss: -35456.73828125]\n",
      "[Epoch 49/100] [Batch 400/782] [D loss: -135.9974365234375] [G loss: -34922.80078125]\n",
      "[Epoch 49/100] [Batch 500/782] [D loss: -5860.669921875] [G loss: -30522.74609375]\n",
      "[Epoch 49/100] [Batch 600/782] [D loss: -184493.859375] [G loss: 308281.15625]\n",
      "[Epoch 49/100] [Batch 700/782] [D loss: 10228.623046875] [G loss: -53025.640625]\n",
      "[Epoch 50/100] [Batch 0/782] [D loss: -813.916748046875] [G loss: -52972.7109375]\n",
      "[Epoch 50/100] [Batch 100/782] [D loss: 6463.32421875] [G loss: -54329.375]\n",
      "[Epoch 50/100] [Batch 200/782] [D loss: -849.38525390625] [G loss: -45370.5]\n",
      "[Epoch 50/100] [Batch 300/782] [D loss: -13251.392578125] [G loss: 67529.890625]\n",
      "[Epoch 50/100] [Batch 400/782] [D loss: -23883.90625] [G loss: -273789.875]\n",
      "[Epoch 50/100] [Batch 500/782] [D loss: -294889.90625] [G loss: 309566.75]\n",
      "[Epoch 50/100] [Batch 600/782] [D loss: -733170.3125] [G loss: 375475.46875]\n",
      "[Epoch 50/100] [Batch 700/782] [D loss: -774126.125] [G loss: 393150.1875]\n",
      "[Epoch 51/100] [Batch 0/782] [D loss: -799587.5625] [G loss: 402306.0]\n",
      "[Epoch 51/100] [Batch 100/782] [D loss: -815056.5] [G loss: 409496.4375]\n",
      "[Epoch 51/100] [Batch 200/782] [D loss: -827229.9375] [G loss: 415513.4375]\n",
      "[Epoch 51/100] [Batch 300/782] [D loss: -844224.0625] [G loss: 421574.4375]\n",
      "[Epoch 51/100] [Batch 400/782] [D loss: 5894.5322265625] [G loss: -51624.63671875]\n",
      "[Epoch 51/100] [Batch 500/782] [D loss: -248443.484375] [G loss: -67083.796875]\n",
      "[Epoch 51/100] [Batch 600/782] [D loss: -32629.5859375] [G loss: 15362.853515625]\n",
      "[Epoch 51/100] [Batch 700/782] [D loss: -834681.625] [G loss: 421272.5]\n",
      "[Epoch 52/100] [Batch 0/782] [D loss: -9991.85546875] [G loss: 386071.96875]\n",
      "[Epoch 52/100] [Batch 100/782] [D loss: -39630.35546875] [G loss: 269354.375]\n",
      "[Epoch 52/100] [Batch 200/782] [D loss: -107065.7890625] [G loss: -238699.4375]\n",
      "[Epoch 52/100] [Batch 300/782] [D loss: -528915.625] [G loss: 329236.375]\n",
      "[Epoch 52/100] [Batch 400/782] [D loss: -4370.462890625] [G loss: 243985.625]\n",
      "[Epoch 52/100] [Batch 500/782] [D loss: -3859.01025390625] [G loss: -379073.75]\n",
      "[Epoch 52/100] [Batch 600/782] [D loss: -694497.5625] [G loss: 399967.59375]\n",
      "[Epoch 52/100] [Batch 700/782] [D loss: 10806.6298828125] [G loss: -56847.74609375]\n",
      "[Epoch 53/100] [Batch 0/782] [D loss: 542032.25] [G loss: -80431.390625]\n",
      "[Epoch 53/100] [Batch 100/782] [D loss: 3650.6552734375] [G loss: -63884.1796875]\n",
      "[Epoch 53/100] [Batch 200/782] [D loss: 2473.093994140625] [G loss: -59844.40234375]\n",
      "[Epoch 53/100] [Batch 300/782] [D loss: 5057.705078125] [G loss: -58625.7109375]\n",
      "[Epoch 53/100] [Batch 400/782] [D loss: 1028.03076171875] [G loss: -56267.953125]\n",
      "[Epoch 53/100] [Batch 500/782] [D loss: 571.375244140625] [G loss: -54698.5234375]\n",
      "[Epoch 53/100] [Batch 600/782] [D loss: 216.3978271484375] [G loss: -50538.671875]\n",
      "[Epoch 53/100] [Batch 700/782] [D loss: 3024.627685546875] [G loss: -40832.39453125]\n",
      "[Epoch 54/100] [Batch 0/782] [D loss: 183231.546875] [G loss: -35999.84375]\n",
      "[Epoch 54/100] [Batch 100/782] [D loss: 1932.72998046875] [G loss: -44234.29296875]\n",
      "[Epoch 54/100] [Batch 200/782] [D loss: 1326.381591796875] [G loss: -45534.84375]\n",
      "[Epoch 54/100] [Batch 300/782] [D loss: 1805.5352783203125] [G loss: -46546.8125]\n",
      "[Epoch 54/100] [Batch 400/782] [D loss: 656.3628540039062] [G loss: -47218.0390625]\n",
      "[Epoch 54/100] [Batch 500/782] [D loss: 743.1707763671875] [G loss: -47643.8828125]\n",
      "[Epoch 54/100] [Batch 600/782] [D loss: 1144.193603515625] [G loss: -48131.078125]\n",
      "[Epoch 54/100] [Batch 700/782] [D loss: -56.2470703125] [G loss: -49028.203125]\n",
      "[Epoch 55/100] [Batch 0/782] [D loss: 1353.3082275390625] [G loss: -49643.1875]\n",
      "[Epoch 55/100] [Batch 100/782] [D loss: -670.140869140625] [G loss: -48688.375]\n",
      "[Epoch 55/100] [Batch 200/782] [D loss: -679.5922241210938] [G loss: -46197.578125]\n",
      "[Epoch 55/100] [Batch 300/782] [D loss: -78671.1484375] [G loss: -80985.140625]\n",
      "[Epoch 55/100] [Batch 400/782] [D loss: -27101.0390625] [G loss: -321447.375]\n",
      "[Epoch 55/100] [Batch 500/782] [D loss: -35516.359375] [G loss: -100377.828125]\n",
      "[Epoch 55/100] [Batch 600/782] [D loss: 12518.1171875] [G loss: -414255.1875]\n",
      "[Epoch 55/100] [Batch 700/782] [D loss: -582252.1875] [G loss: 247497.890625]\n",
      "[Epoch 56/100] [Batch 0/782] [D loss: 47554.86328125] [G loss: -43896.8828125]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 145\u001b[0m\n\u001b[0;32m    142\u001b[0m discriminator \u001b[38;5;241m=\u001b[39m Discriminator()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m# Training on CIFAR-10\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m \u001b[43mtrain_wgan_gp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader_cifar10\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS_CIFAR10\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcifar10\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# Fine-tuning on STL-10\u001b[39;00m\n\u001b[0;32m    148\u001b[0m train_wgan_gp(generator, discriminator, trainloader_stl10, EPOCHS_STL10, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstl10\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 102\u001b[0m, in \u001b[0;36mtrain_wgan_gp\u001b[1;34m(generator, discriminator, dataloader, epochs, save_interval, phase)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (imgs, _) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m--> 102\u001b[0m         real_imgs \u001b[38;5;241m=\u001b[39m \u001b[43mimgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m real_imgs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    105\u001b[0m         \u001b[38;5;66;03m# ---------------------\u001b[39;00m\n\u001b[0;32m    106\u001b[0m         \u001b[38;5;66;03m#  Train Discriminator\u001b[39;00m\n\u001b[0;32m    107\u001b[0m         \u001b[38;5;66;03m# ---------------------\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# Define constants\n",
    "IMG_SIZE = 64\n",
    "LATENT_DIM = 100\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS_CIFAR10 = 100\n",
    "EPOCHS_STL10 = 100\n",
    "LEARNING_RATE = 0.0002\n",
    "CRITIC_ITERATIONS = 5\n",
    "LAMBDA_GP = 10\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.init_size = IMG_SIZE // 4\n",
    "        self.l1 = nn.Sequential(nn.Linear(LATENT_DIM, 128 * self.init_size ** 2))\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 3, 3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.l1(z)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512 * 4 * 4, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        validity = self.model(img)\n",
    "        return validity\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "trainset_cifar10 = datasets.CIFAR10(root='../../data/cifar10', train=True, download=True, transform=transform)\n",
    "trainloader_cifar10 = DataLoader(trainset_cifar10, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "trainset_stl10 = datasets.STL10(root='../../data/stl10', split='train+unlabeled', download=True, transform=transform)\n",
    "trainloader_stl10 = DataLoader(trainset_stl10, batch_size=BATCH_SIZE, shuffle=True)\n",
    "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    alpha = torch.randn((real_samples.size(0), 1, 1, 1), device=device)\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    d_interpolates = D(interpolates)\n",
    "    fake = torch.ones(real_samples.size(0), 1, requires_grad=False, device=device)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "def train_wgan_gp(generator, discriminator, dataloader, epochs, save_interval, phase):\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=0.0005, betas=(0.5, 0.999))\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i, (imgs, _) in enumerate(dataloader):\n",
    "            real_imgs = imgs.to(device)\n",
    "            batch_size = real_imgs.size(0)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "            for _ in range(CRITIC_ITERATIONS):\n",
    "                optimizer_D.zero_grad()\n",
    "\n",
    "                z = torch.randn(batch_size, LATENT_DIM).to(device)\n",
    "                fake_imgs = generator(z)\n",
    "\n",
    "                real_validity = discriminator(real_imgs)\n",
    "                fake_validity = discriminator(fake_imgs.detach())\n",
    "                gradient_penalty = compute_gradient_penalty(discriminator, real_imgs.data, fake_imgs.data)\n",
    "                d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + LAMBDA_GP * gradient_penalty\n",
    "\n",
    "                d_loss.backward()\n",
    "                optimizer_D.step()\n",
    "\n",
    "            # -----------------\n",
    "            #  Train Generator\n",
    "            # -----------------\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            gen_imgs = generator(z)\n",
    "            fake_validity = discriminator(gen_imgs)\n",
    "            g_loss = -torch.mean(fake_validity)\n",
    "\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f\"[Epoch {epoch+1}/{epochs}] [Batch {i}/{len(dataloader)}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}]\")\n",
    "\n",
    "        if epoch % save_interval == 0:\n",
    "            save_image(gen_imgs.data[:25], f\"images/{phase}_{epoch}_2.png\", nrow=5, normalize=True)\n",
    "            torch.save(generator.state_dict(), f\"saved_model/generator2_{phase}_{epoch}.pt\")\n",
    "# Instantiate models\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "# Training on CIFAR-10\n",
    "train_wgan_gp(generator, discriminator, trainloader_cifar10, EPOCHS_CIFAR10, 5, 'cifar10')\n",
    "\n",
    "# Fine-tuning on STL-10\n",
    "train_wgan_gp(generator, discriminator, trainloader_stl10, EPOCHS_STL10, 5, 'stl10')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
